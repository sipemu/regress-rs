# R Validation Report

This document describes how the Rust `anofox-regression` library is validated against R to ensure numerical accuracy and correctness of statistical implementations.

## Overview

The validation system uses R as the reference oracle to validate Rust implementations through a Test-Driven Development approach. Reference values are generated using R's established statistical packages with fixed random seeds to ensure reproducibility.

## Core Architecture

The validation system uses a three-component structure:

1. **Reference Generation**: R scripts generate expected values using `set.seed(42)` for reproducibility
2. **Test Data Storage**: Generated data is stored in the `validation/` folder and embedded in Rust test files
3. **Rust Test Suites**: Integration tests load reference data and verify numerical agreement between implementations

## Key Components

### Reference Generation Scripts

| Script | Purpose |
|--------|---------|
| `validation/generate_validation_data.R` | Generates OLS, Ridge, Elastic Net, WLS, and diagnostic test cases |
| `tests/r_scripts/generate_regression_validation.R` | Validates WLS, Ridge, Elastic Net, and Tweedie regressors |
| `tests/r_scripts/generate_glm_validation.R` | Validates GLM implementations (Poisson, Binomial, Gamma, etc.) |
| `tests/r_scripts/generate_alm_validation.R` | Validates ALM implementations (7 core distributions) |
| `tests/r_scripts/generate_alm_validation_extended.R` | Validates ALM extended distributions (14 additional) |
| `tests/r_scripts/generate_alm_loss_validation.R` | Validates ALM loss function implementations |
| `tests/r_scripts/generate_aid_validation.R` | Validates AID demand classification |

### R Packages Used

- **base stats**: `lm()`, `glm()` for core regression and GLM
- **MASS**: `glm.nb()` for negative binomial regression
- **glmnet**: Ridge regression, Elastic Net, Lasso with coordinate descent
- **statmod**: Tweedie family distributions, inverse Gaussian
- **greybox**: `alm()` for augmented linear models, `aid()` for demand identification

## Validation Categories

### 1. Ordinary Least Squares (OLS)

Validates coefficients, standard errors, t-statistics, p-values, R², adjusted R², F-statistic, AIC, BIC, log-likelihood, and residuals.

**Tolerance**: `1e-8` for coefficients and standard errors

**Test cases**:
- Simple linear regression (n=20, p=1)
- Multiple regression (n=50, p=2)

### 2. Weighted Least Squares (WLS)

Validates coefficient estimation with observation weights for heteroscedastic data.

**Tolerance**: `0.01` for coefficients

**Test cases**:
- Weights inversely proportional to variance (1/x²)
- Comparison with OLS on same data

### 3. Ridge Regression

Validates L2-penalized regression with closed-form solution: β = (X'X + λI)⁻¹X'y

**Tolerance**: `0.01` for coefficients

**Test cases**:
- λ = 0 (should match OLS)
- λ = 0.1, 1.0, 10.0 (increasing shrinkage)
- Collinear data (VIF > 16000)

### 4. Elastic Net

Validates combined L1+L2 penalty using coordinate descent optimization.

**Objective**: ||y - Xβ||² + λ(1-α)||β||² + λα||β||₁

**Tolerance**: `0.2` for coefficients (coordinate descent may produce slight differences)

**Test cases**:
- α = 0.5, λ = 0.3 (balanced penalty)
- α = 1.0 (Lasso)
- Sparse coefficient recovery

### 5. Poisson GLM

Validates count data regression with log, identity, and sqrt link functions.

**Tolerance**: `0.01` for coefficients, `0.1` for deviance

**Test cases**:
- Log link (standard Poisson)
- Identity link
- With offset (exposure modeling)

### 6. Binomial GLM

Validates binary/proportion response regression with multiple link functions.

**Tolerance**: `0.1` for coefficients, `0.5` for deviance

**Test cases**:
- Logit link (logistic regression)
- Probit link
- Complementary log-log link

### 7. Tweedie GLM

Validates exponential dispersion models covering Poisson, Gamma, and Inverse Gaussian.

**Tolerance**: `0.05` - `0.2` depending on variance power

**Test cases**:
- Gamma (var.power = 2)
- Poisson (var.power = 1)
- Inverse Gaussian (var.power = 3)
- Compound Poisson-Gamma (var.power = 1.5)

### 8. Negative Binomial GLM

Validates overdispersed count data regression with theta parameter.

**Tolerance**: `0.05` for coefficients

**Test cases**:
- Fixed theta (from R's `glm.nb()`)
- Theta estimation
- High theta approaching Poisson

### 9. RLS (Recursive Least Squares)

Validates online/streaming regression that updates coefficients incrementally.

**Tolerance**: `0.01` - `0.1` (compared to batch OLS)

**Test cases**:
- Convergence to OLS with forgetting factor = 1.0
- Forgetting factor behavior (exponential weighting of recent data)
- Online updates

### 10. BLS/NNLS (Bounded/Non-Negative Least Squares)

Validates constrained regression with coefficient bounds.

**Tolerance**: `0.1` for coefficients

**Test cases**:
- NNLS (all coefficients ≥ 0) vs R's `nnls` package
- Box constraints with arbitrary lower/upper bounds
- Comparison with OLS when OLS solution satisfies constraints

### 11. Regression Diagnostics

Validates model diagnostic statistics against R's `lm()` influence measures.

**Tolerance**: `1e-6` to `1e-8`

**Metrics validated**:
- Leverage (hat values)
- Cook's distance
- Studentized residuals
- Variance Inflation Factor (VIF)

### 12. ALM (Augmented Linear Model)

Validates maximum likelihood estimation for 21 distribution families against R's `greybox::alm()` function.

**Tolerance**: `0.15` for coefficients, `0.20` for log-likelihood

**Distributions validated** (grouped by data type):

| Category | Distributions | Link Function | Status |
|----------|---------------|---------------|--------|
| Symmetric Continuous | Normal, Laplace, Logistic, StudentT | Identity | ✓ All validated |
| Robust | GeneralisedNormal | Identity | ✓ Validated |
| Robust | AsymmetricLaplace, S | Identity | ⏳ Pending |
| Log-domain | LogNormal, LogLaplace, LogGeneralisedNormal | Log | ✓ All validated |
| Positive Continuous | Gamma, Exponential | Log | ✓ Validated |
| Positive Continuous | InverseGaussian | Log | ⏳ Pending |
| Unit Interval (0,1) | LogitNormal | Identity* | ✓ Validated |
| Unit Interval (0,1) | Beta | Logit | ⏳ Needs dual-predictor |
| Zero-inflated | FoldedNormal, RectifiedNormal | Identity | ⏳ IRLS differences |
| Transform | BoxCoxNormal | Identity (transformed) | ⏳ Pending |
| Count | Poisson, Geometric | Log | ✓ Validated |
| Count | NegativeBinomial, Binomial | Log/Logit | ⏳ Pending |
| Cumulative | CumulativeLogistic, CumulativeNormal | Logit/Probit | ⏳ Needs ordinal model |

*LogitNormal uses Identity link on logit-scale location parameter (R greybox parameterization)

**Test cases**:
- Each distribution with n=50 observations
- Simple linear regression (1 predictor + intercept)
- Validates intercept, coefficient, scale parameter, and log-likelihood

### 13. AID (Automatic Identification of Demand)

Validates demand classification against R's `greybox::aid()` function.

**Components validated**:
- Demand type classification (Regular vs Intermittent)
- Fractional vs count data detection
- New product detection (leading zeros)
- Obsolete product detection (trailing zeros)
- Stockout detection (unexpected zeros)
- Information criteria (AIC, BIC, AICc)

**Test cases**:
- Regular count demand (Poisson-like, 0% zeros)
- Regular fractional demand (Normal-like)
- Intermittent count demand (65% zeros)
- Intermittent fractional demand (39% zeros)
- New product (30 leading zeros)
- Obsolete product (30 trailing zeros)
- Stockouts (3 unexpected zeros in middle)
- Overdispersed count (Negative Binomial)
- Skewed positive (Gamma/LogNormal)
- IC comparison (AIC vs BIC vs AICc)

## Test Coverage

| Category | Tests | Tolerance |
|----------|-------|-----------|
| OLS | 15+ | 1e-8 |
| WLS | 5+ | 0.01 |
| Ridge | 10+ | 0.01 |
| Elastic Net | 5+ | 0.2 |
| RLS | 10+ | 0.01-0.1 |
| BLS/NNLS | 5+ | 0.1 |
| Poisson GLM | 15+ | 0.01 |
| Binomial GLM | 10+ | 0.1 |
| Tweedie GLM | 10+ | 0.05-0.2 |
| Negative Binomial | 8+ | 0.05 |
| Diagnostics | 10+ | 1e-6 |
| ALM | 21+ | 0.15 |
| AID | 12+ | - |
| **Total** | **364+** | - |

## Reproducibility

All validation is reproducible through:

1. **Fixed random seeds**: All R scripts use `set.seed(42)`
2. **Version-controlled data**: Reference output stored in `validation/validation_output.txt`
3. **CI/CD verification**: Tests run automatically on every commit
4. **Transparent documentation**: R code embedded in Rust test comments

## Running Validation

### Regenerate R References

```bash
cd validation
Rscript generate_validation_data.R > validation_output.txt
```

### Run All Rust Tests

```bash
cargo test
```

### Run Specific Validation Tests

```bash
# OLS validation
cargo test test_ols_simple_vs_r

# GLM validation
cargo test r_validation

# Diagnostic validation
cargo test test_leverage
cargo test test_cooks_distance
```

## Implementation Notes

### Tolerance Choices Explained

The tolerance levels used in validation tests vary significantly across different regression methods. This section explains why certain methods require looser tolerances than others.

#### OLS (Tolerance: 1e-8)

OLS uses a **closed-form solution** via QR decomposition: β = (X'X)⁻¹X'y. Both R and this library use numerically stable QR factorization, producing nearly identical results down to floating-point precision. The only differences arise from minor variations in LAPACK implementations.

#### WLS (Tolerance: 0.01)

While WLS also has a closed-form solution (weighted normal equations), the tolerance is slightly looser because:
- Extreme weight ratios (e.g., 1/x² weights spanning 1.0 to 0.001) can amplify small numerical differences
- R and Rust may handle near-zero weights differently in edge cases
- Standard error calculations involve the weighted residual variance, which accumulates small rounding errors

#### Ridge Regression (Tolerance: 0.01)

Ridge has a closed-form solution: β = (X'X + λI)⁻¹X'y. The moderate tolerance accounts for:
- Different implementations of the regularization term (some add λ to diagonal before inversion, others use SVD)
- **Lambda scaling conventions**: R's `glmnet` uses λ/n scaling by default, while this library uses raw λ. Tests must account for this difference
- Intercept handling: Whether the intercept is penalized affects final coefficients

#### Elastic Net (Tolerance: 0.2)

Elastic Net requires the **largest tolerance** because it uses **coordinate descent optimization**, an iterative algorithm with no closed-form solution:

1. **Algorithm differences**: R's `glmnet` uses a highly optimized coordinate descent with warm starts and active set strategies. This library implements a standard coordinate descent that may converge to slightly different local optima
2. **Convergence criteria**: Different stopping rules (relative vs. absolute tolerance, coefficient change vs. objective function change) lead to different final solutions
3. **Soft-thresholding**: The L1 penalty creates non-smooth optimization landscapes where multiple solutions may be equally valid within numerical precision
4. **Cycling order**: The order in which coordinates are updated can affect the final solution
5. **Initialization**: Different starting points can lead to different convergence paths

Despite these differences, both implementations produce statistically equivalent models with similar predictive performance.

#### Poisson GLM (Tolerance: 0.01 coefficients, 0.1 deviance)

Poisson GLM uses **Iteratively Reweighted Least Squares (IRLS)**, which introduces several sources of numerical variation:

1. **Convergence criteria**: R's `glm()` and this library use different stopping rules
2. **Starting values**: Initial coefficient estimates affect convergence path
3. **Step halving**: Different line search strategies when IRLS overshoots
4. **Link function derivatives**: Small differences in computing the working weights

The deviance tolerance is larger because it accumulates differences across all observations.

#### Binomial GLM (Tolerance: 0.1 coefficients, 0.5 deviance)

Binomial GLM has **larger tolerances** than Poisson because:

1. **Boundary issues**: Probabilities near 0 or 1 require careful numerical handling to avoid log(0)
2. **Separation**: Near-complete separation in data can cause coefficient inflation, handled differently across implementations
3. **Link-specific sensitivity**:
   - **Logit**: Most stable, tolerance ~0.1
   - **Probit**: Involves normal CDF, tolerance ~0.1
   - **Cloglog**: Most numerically sensitive (involves exp(exp(x))), tolerance ~0.5

#### Tweedie GLM (Tolerance: 0.05-0.2)

Tweedie models span multiple distributions with varying numerical stability:

| Variance Power | Distribution | Tolerance | Reason |
|----------------|--------------|-----------|--------|
| p = 1 | Poisson | 0.01 | Well-conditioned |
| p = 2 | Gamma | 0.05 | Log link, moderate sensitivity |
| p = 3 | Inverse Gaussian | 0.1-0.2 | Highly sensitive to outliers |
| 1 < p < 2 | Compound Poisson-Gamma | 0.2 | Complex density, deviance approximations differ |

The deviance calculation for Tweedie distributions involves special functions that may be implemented differently.

#### Negative Binomial GLM (Tolerance: 0.05 coefficients)

Negative binomial requires moderate tolerance due to:

1. **Theta estimation**: The dispersion parameter θ is estimated jointly with coefficients using alternating optimization. R's `glm.nb()` uses profile likelihood while this library uses moment estimation, leading to different θ values
2. **Theta sensitivity**: Small differences in θ propagate to coefficient estimates
3. **Variance function**: V(μ) = μ + μ²/θ means the working weights depend heavily on θ

#### RLS (Tolerance: 0.01-0.1)

Recursive Least Squares is an **online algorithm** that processes data sequentially:

1. **P matrix initialization**: RLS starts with an initial covariance matrix P₀ = δI. The choice of δ affects early estimates and creates differences from batch OLS
2. **Forgetting factor**: With λ < 1, RLS exponentially downweights older observations, intentionally diverging from OLS
3. **Numerical accumulation**: Sequential updates accumulate small rounding errors over many iterations
4. **Convergence behavior**: Even with λ = 1, RLS converges to OLS asymptotically but may not exactly match for finite samples

#### BLS/NNLS (Tolerance: 0.1)

Bounded Least Squares uses **active set methods** to handle inequality constraints:

1. **Active set identification**: Different algorithms may identify slightly different active constraint sets
2. **Degeneracy**: When multiple constraints are nearly active, small numerical differences can change which constraints are binding
3. **R's `nnls` package**: Uses the Lawson-Hanson algorithm; this library uses a similar but not identical implementation
4. **Constraint boundaries**: Solutions on constraint boundaries are sensitive to numerical precision

#### Diagnostics (Tolerance: 1e-6)

Diagnostic statistics use **direct matrix computations** without iteration:
- Leverage: H = X(X'X)⁻¹X' diagonal elements
- Cook's distance: Closed-form using leverage and residuals
- Studentized residuals: Direct formula using MSE and leverage

The tight tolerance reflects that these are deterministic calculations.

#### ALM (Tolerance: 0.15 coefficients, 0.20 log-likelihood)

ALM (Augmented Linear Model) uses **IRLS with distribution-specific likelihood functions**:

1. **Distribution diversity**: 24 distributions with varying complexity (Normal to BoxCoxNormal)
2. **Optimizer differences**: R's `greybox` uses specific optimization strategies that may differ
3. **Scale estimation**: Different methods for estimating scale parameters (MLE vs method of moments)
4. **Link function handling**: Some distributions (Beta, Binomial) use non-identity links
5. **Extra parameters**: Distributions like GeneralisedNormal and BoxCoxNormal have shape/lambda parameters

**Currently validated distributions (13)**: Normal, Laplace, StudentT, Logistic, LogNormal, Poisson, Gamma, Exponential, GeneralisedNormal, Geometric, LogitNormal, LogLaplace, LogGeneralisedNormal

**Pending investigation (11)**: FoldedNormal, RectifiedNormal, Beta, S, BoxCoxNormal, CumulativeLogistic, CumulativeNormal, NegativeBinomial, Binomial, InverseGaussian, AsymmetricLaplace

**Key fixes implemented for R compatibility**:
- **Geometric**: Changed from Logit link to Log link, modeling mean λ = (1-p)/p instead of probability p
- **LogitNormal**: Changed from Logit link to Identity link, modeling logit-scale location parameter directly
- **LogLaplace**: Fixed scale estimation and IRLS weights to use log-space residuals
- **LogGeneralisedNormal**: Fixed scale estimation to use log-space residuals, corrected likelihood coefficient

**Remaining investigations needed**:
- **Beta**: Requires dual-predictor architecture (R models both α and β shape parameters)
- **Cumulative distributions**: Require ordinal regression (proportional odds model)
- **S distribution**: R greybox uses HAM-minimization approach with specific parameterization
- **FoldedNormal/RectifiedNormal**: IRLS convergence differences with R implementation

The pending distributions have differences in link function parameterization or model structure compared to R greybox. Tests are included but marked as `#[ignore]` for future investigation.

The relatively large tolerance (15%) allows for optimizer implementation differences while ensuring statistical equivalence.

#### AID (No numeric tolerance - classification based)

AID (Automatic Identification of Demand) is primarily a **classification algorithm**:

1. **Demand type**: Binary classification (Regular vs Intermittent) based on zero proportion threshold
2. **Distribution selection**: Best distribution chosen by information criterion
3. **Anomaly detection**: Boolean flags for new product, obsolete, stockouts

Tests validate classification correctness rather than numeric precision. IC values are compared with 10% tolerance.

### Summary Table

| Method | Solution Type | Key Challenge | Tolerance |
|--------|---------------|---------------|-----------|
| OLS | Closed-form (QR) | Floating-point precision | 1e-8 |
| WLS | Closed-form (weighted QR) | Extreme weight ratios | 0.01 |
| Ridge | Closed-form (regularized) | Lambda scaling conventions | 0.01 |
| Elastic Net | Iterative (coordinate descent) | Non-convex, algorithm differences | 0.2 |
| RLS | Sequential (online) | P matrix initialization, accumulation | 0.01-0.1 |
| BLS/NNLS | Iterative (active set) | Constraint boundary sensitivity | 0.1 |
| Poisson GLM | Iterative (IRLS) | Convergence criteria | 0.01-0.1 |
| Binomial GLM | Iterative (IRLS) | Boundary handling, link sensitivity | 0.1-0.5 |
| Tweedie GLM | Iterative (IRLS) | Variance power sensitivity | 0.05-0.2 |
| Negative Binomial | Iterative (IRLS + theta) | Joint estimation | 0.05 |
| Diagnostics | Closed-form (matrix) | Direct computation | 1e-6 |
| ALM | Iterative (IRLS + MLE) | Distribution diversity, scale estimation | 0.15 |
| AID | Classification | Zero proportion threshold | Classification |

### Known Differences from R

1. **Log-likelihood formula**: R uses `RSS/n` in the log-likelihood calculation while this library uses `RSS/(n-p)` (MSE), causing small AIC/BIC differences
2. **Lambda scaling**: `glmnet` uses λ/n scaling by default; tests adjust accordingly
3. **Coordinate descent**: Elastic Net convergence may differ slightly from `glmnet`

## Reference

For the R code used to generate validation data, see:
- `validation/generate_validation_data.R`
- `tests/r_scripts/*.R`
